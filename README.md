# ğŸ§  NLP QA Project with LLaMA 3.2 1B & GPT-2 Large

## ğŸ“Œ Project Overview
This project focuses on developing a **Question-Answering (QA) system** by fine-tuning **LLaMA 3.2 1B** and **GPT-2 Large** on a dataset containing **140,000 Python-related QA pairs**. Our goal is to improve **model accuracy, efficiency, and contextual understanding** in code-related queries.

## ğŸ“Š Dataset
We use the **Glaive Code Assistant Dataset** from [Kaggle](https://www.kaggle.com/datasets/thedevastator/glaive-python-code-qa-dataset), which is designed for training intelligent Python assistants.

### ğŸ”¹ Key Features:
âœ… **140,000+ questions & solutions** related to Python programming.  
âœ… **Structured QA format** for fine-tuning NLP models.  
âœ… **60% Python-focused content**, ensuring high relevance for coding queries.  
âœ… **Real-world user queries** covering simple to advanced programming topics.  

## ğŸš€ Models Used
We fine-tune two models:

### ğŸ”· **LLaMA 3.2 1B**
- **Modern Transformer architecture** optimized for NLP tasks.
- **1.24 billion parameters** enabling strong contextual learning.
- **Handles longer context windows** (2048 tokens).

### ğŸ”· **GPT-2 Large**
- **Classic Transformer-based model** (774M parameters).
- **More resource-efficient** than newer architectures.
- **1024-token context window** (shorter than LLaMA but effective for structured queries).

## ğŸ¯ Project Objectives
1ï¸âƒ£ **Fine-tune LLaMA 3.2 1B and GPT-2 Large** for QA.

2ï¸âƒ£ **Optimize hyperparameters** for better efficiency.

3ï¸âƒ£ **Compare both models** in terms of accuracy, coherence, and response quality.

4ï¸âƒ£ **Improve model inference time** using LoRA & QLoRA techniques.

## âš™ï¸ Installation
Clone the repository and navigate into the project directory:
```bash
git clone https://github.com/AnisBenini/FineTuning-QA_Dataset.git
cd FineTuning-QA_Dataset
```

## ğŸ“¥ Dataset Preparation
Download the dataset and preprocess it using the provided scripts:
ğŸ”— [Kaggle Dataset](https://www.kaggle.com/datasets/thedevastator/glaive-python-code-qa-dataset)

## ğŸ‹ï¸ Training the Model
Fine-tune the models using the training script:
```bash
python train.py --model llama3.2-1B 
```

## ğŸ“ˆ Evaluation & Testing
ğŸš§ Currently, the project **lacks a formal evaluation phase**. The next steps include:

âœ… Implementing **unit tests** to validate functionality.  
âœ… Adding **performance metrics** (BLEU, ROUGE, etc.).  
âœ… Benchmarking results **against baseline QA models**.  

## ğŸ¤ Contributing
We welcome contributions! Follow these steps:
1. **Fork the repository**.
2. **Create a feature branch**:
   ```bash
   git checkout -b feature/new-feature
   ```
3. **Commit your changes**:
   ```bash
   git commit -m "Add a new feature"
   ```
4. **Push to GitHub**:
   ```bash
   git push origin feature/new-feature
   ```
5. **Submit a pull request**.

## ğŸ“œ License
This project is licensed under the **MIT License**. See the `LICENSE` file for details.

## ğŸ™ Acknowledgments
Thanks to **Hugging Face**, **Meta AI**, and **Kaggle** for providing open-source NLP resources and datasets!
